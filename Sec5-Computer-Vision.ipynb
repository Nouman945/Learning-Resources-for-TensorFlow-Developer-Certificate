{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Convolution Neural Networks and Computer Vision with Tensorflow\n",
    "\n",
    "- Computer vision is the pratice of writing algorithms which can discover patterens in Visial data. Like a self driving car Camera is capable of recognizing car/objects in front of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data.\n",
    "A very Crucial step at the begining of any machine learning project is to become one with the data. \n",
    "\n",
    "And for computer vision project you need to visualize many samples of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('pizza_steak'):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of listing the files and folders inside a directory\n",
    "\n",
    "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n",
    "num_steak_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To visualize our images let's get the class names programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names promatically\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "data_dir = pathlib.Path(\"pizza_steak/train\")\n",
    "print(data_dir)\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize our images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "def view_random_image(target_dir, target_class):\n",
    "    target_folder = os.path.join(target_dir, target_class)\n",
    "\n",
    "    random_image = random.sample(os.listdir(target_folder),1)\n",
    "\n",
    "    img = mpimg.imread(os.path.join(target_folder,random_image[0]))\n",
    "    plt.imshow(img)\n",
    "    plt.title(target_class)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/train\"\n",
    "target_class = class_names[0]\n",
    "img = view_random_image(target_dir, target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.constant(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the image shape\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Data\n",
    "img/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An end-to-end example \n",
    "\n",
    "Lets build a convolution neural network to find patterens in our data. Steps we need to do\n",
    "- Load our images\n",
    "- Preprocess our images\n",
    "- Build a CNN\n",
    "- compile the CNN\n",
    "- fit the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Setup the data preprocessing ( get all of the pixel values between 0 and 1). also called scalling/normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dir = \"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/train\"\n",
    "test_dir = \"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/train\"\n",
    "\n",
    "# Import Data from Directories and turn it into batches\n",
    "train_data  = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                batch_size=32,\n",
    "                                                target_size=(244,244),\n",
    "                                                class_mode = \"binary\",\n",
    "                                                seed=42)\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(directory=test_dir,\n",
    "                                               batch_size=32,\n",
    "                                               target_size=(244,244),\n",
    "                                               class_mode=\"binary\",\n",
    "                                               seed=42)\n",
    "\n",
    "# Build the CNN model ( same as the TINY VGG on the CNN Expainer website)\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=10,\n",
    "                           kernel_size=3,\n",
    "                           activation='relu',\n",
    "                           input_shape=(244,244,3)),\n",
    "    tf.keras.layers.Conv2D(10,3,activation='relu'), \n",
    "    tf.keras.layers.MaxPool2D(pool_size=2, padding='valid'), # Gives the max of a matrix\n",
    "    tf.keras.layers.Conv2D(10,3, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model_1.fit(train_data,\n",
    "                      epochs=5,\n",
    "                      steps_per_epoch=len(train_data),\n",
    "                      validation_data=valid_data,\n",
    "                      validation_steps=len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data) # batch size 1500/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Create the neural netword from previous section to see how it performs on the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(244,244,3)),\n",
    "    tf.keras.layers.Dense(4,activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model_2.fit(train_data,epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to Improve the model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# create the model\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(244,244,3)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                metrics=['accuracy']) \n",
    "\n",
    "# Fit the model\n",
    "model_3.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "Model 1 have 36,651 parameters as compared to model 3 with 17,881,201. 487 times larger.\n",
    "\n",
    "That's why CNN's are used for image classification problems.\n",
    "\n",
    "But Sometimes dense network can perform better too on the image data but that depends on the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17881201/36651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "img1 = view_random_image(\"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/test\", \"pizza\")\n",
    "plt.subplot(1,2,2)\n",
    "img2 = view_random_image(\"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/test\",\"steak\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/train\"\n",
    "test_dir = \"/home/abdulrahman/Documents/TensorFlow-Developer-Certificate-Guide/pizza_steak/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                               target_size=(244,244),\n",
    "                                               class_mode='binary',\n",
    "                                               batch_size=32)\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                             target_size=(244,244),\n",
    "                                             class_mode='binary',\n",
    "                                             batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of train batch size\n",
    "images, labels = train_data.next()\n",
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Batches are there? \n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the image tensor look like?\n",
    "images[:2], images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Activation\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential([\n",
    "    Conv2D(filters=10,\n",
    "           kernel_size=3,\n",
    "           strides=1,\n",
    "           padding=\"valid\",\n",
    "           activation=\"relu\",\n",
    "           input_shape=(244,244,3)),\n",
    "    Conv2D(10,3, activation='relu'),\n",
    "    Conv2D(10,3, activation='relu'),\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='binary_crossentropy',\n",
    "                optimizer=Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary of the model\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = model_4.fit(train_data,\n",
    "                        epochs=5,\n",
    "                        steps_per_epoch=len(train_data),\n",
    "                        validation_data=test_data,\n",
    "                        validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the training Curves\n",
    "import pandas as pd\n",
    "pd.DataFrame(history_4.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training curves seperately\n",
    "def plot_loss_curves(history):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    accuracy = history.history[\"accuracy\"]\n",
    "    val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "    epochs = range(len(history.history[\"loss\"]))\n",
    "\n",
    "    plt.plot(epochs, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Val Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label=\"Accuracy\")\n",
    "    plt.plot(epochs, val_accuracy, label=\"Val Accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the model parameters.\n",
    "\n",
    "Fitting a machine learning model comes in 3 stages.\n",
    "\n",
    "1. Creating a baseline.\n",
    "2. Beat the baseline by overfitting a larger model.\n",
    "3. Reduce Overfitting.\n",
    "\n",
    "Ways to induce overfitting.\n",
    "\n",
    "1. Increase the number of conv layers\n",
    "2. increase the numbers of conv filteres\n",
    "3. Add more Dense layers to the output of flattened layer.\n",
    "\n",
    "Reduce Overfitting.\n",
    "\n",
    "1. Add data Augmentation\n",
    "2. Add regularization layers ( Such as MaxPool2D )\n",
    "3. Add more data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential([\n",
    "    Conv2D(10,3, activation='relu', input_shape=(244,244,3)),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5 = model_5.fit(train_data,\n",
    "            epochs=5,\n",
    "            steps_per_epoch=len(train_data),\n",
    "            validation_data=test_data,\n",
    "            validation_steps=len(test_data)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_loss_curves(history_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ImageDataGenerator training instance with data augumentation\n",
    "\n",
    "# Create ImageDataGenerator training instance with data augmentation\n",
    "train_datagen_aug = ImageDataGenerator(rescale=1/255.,\n",
    "                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                             shear_range=0.2, # shear the image\n",
    "                                             zoom_range=0.2, # zoom into the image\n",
    "                                             width_shift_range=0.2, # shift the image width ways\n",
    "                                             height_shift_range=0.2, # shift the image height ways\n",
    "                                             horizontal_flip=True) # flip the image on the horizontal axis\n",
    "\n",
    "# Create ImageDataGenerator training instance without data augmentation\n",
    "train_datagenn = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "test_datagne = ImageDataGenerator(rescale=1/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and augment it from training directory\n",
    "print(\"Augmented training images:\")\n",
    "train_data_augmented = train_datagen_aug.flow_from_directory(train_dir,\n",
    "                                                                   target_size=(224, 224),\n",
    "                                                                   batch_size=32,\n",
    "                                                                   class_mode='binary',\n",
    "                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n",
    "# Create non-augmented data batches\n",
    "print(\"Non-augmented training images:\")\n",
    "train_data = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='binary',\n",
    "                                               shuffle=False) # Don't shuffle for demonstration purposes\n",
    "\n",
    "print(\"Unchanged test images:\")\n",
    "test_data = test_datagen.flow_from_directory(test_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             batch_size=32,\n",
    "                                             class_mode='binary')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some of the augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_data.next()\n",
    "\n",
    "aug_images, aug_labels = train_data_augmented.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original image and augmented image\n",
    "import random\n",
    "random_number = random.randint(0,32)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "print(f\"Showing Image {random_number}\")\n",
    "plt.imshow(images[random_number])\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(False)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(aug_images[random_number])\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.axis(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "model_6 = Sequential([\n",
    "    Conv2D(10,3,activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_6.compile(loss='binary_crossentropy',\n",
    "                optimizer=Adam(),\n",
    "                metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on Augmented training data\n",
    "history_6 = model_6.fit(train_data_augmented, epochs=5, \n",
    "            steps_per_epoch=len(train_data_augmented),\n",
    "            validation_data=test_data,\n",
    "            validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets shuffle our training data and aumented trainig data and train another model same as before and check the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and augument it and shuffle it from the training directory\n",
    "train_data_aug_shuffled =  train_datagen_aug.flow_from_directory(train_dir,\n",
    "                                                                target_size=(224,224),\n",
    "                                                                batch_size=32,\n",
    "                                                                class_mode=\"binary\",\n",
    "                                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model_7 = Sequential([\n",
    "    Conv2D(10,3, activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(10,3,activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_7.compile(loss='binary_crossentropy',\n",
    "                optimizer=Adam(),\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model\n",
    "history_7 = model_7.fit(train_data_aug_shuffled,\n",
    "                        epochs=5,\n",
    "                        steps_per_epoch=len(train_data_aug_shuffled),\n",
    "                        validation_data=test_data,\n",
    "                        validation_steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions using our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
